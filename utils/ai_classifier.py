"""
HuggingFace Optimized AI Classifier for Iranian Legal Archive System
Provides Persian BERT classification with fallback rule-based system
"""

import time
import ssl
import logging
import re
from typing import Dict, Any, List, Optional
from pathlib import Path
try:
    from hazm import Normalizer
    HAZM_AVAILABLE = True
except ImportError:
    HAZM_AVAILABLE = False
    logging.warning("hazm not available, Persian normalization disabled")
    
    # Fallback normalizer
    class Normalizer:
        def normalize(self, text):
            return text

try:
    import torch
    from transformers import pipeline, AutoTokenizer, AutoModel
    from sentence_transformers import SentenceTransformer
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    logging.warning("Transformers not available, using rule-based classification only")

from .legal_sources import OPTIMIZED_MODELS, COMPREHENSIVE_LEGAL_TERMS

logger = logging.getLogger(__name__)

# Enhanced paths
DATA_DIR = Path("/tmp/data")
MODELS_CACHE_PATH = DATA_DIR / "models_cache"
MODELS_CACHE_PATH.mkdir(parents=True, exist_ok=True)


class HuggingFaceOptimizedClassifier:
    """Ø³ÛŒØ³ØªÙ… Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Hugging Face"""
    
    def __init__(self, cache_system=None):
        self.cache_system = cache_system
        self.models = {}
        self.tokenizers = {}
        self.is_ready = False
        self.load_attempts = 0
        self.max_attempts = 2
        self.normalizer = Normalizer()
        
        # Enhanced legal categories
        self.enhanced_categories = {
            'Ù‚Ø§Ù†ÙˆÙ†_Ø§Ø³Ø§Ø³ÛŒ': {
                'keywords': ['Ù‚Ø§Ù†ÙˆÙ† Ø§Ø³Ø§Ø³ÛŒ', 'Ø§ØµÙˆÙ„ Ø§Ø³Ø§Ø³ÛŒ', 'Ù…Ø¬Ù„Ø³ Ø´ÙˆØ±Ø§ÛŒ Ø§Ø³Ù„Ø§Ù…ÛŒ', 'Ø´ÙˆØ±Ø§ÛŒ Ù†Ú¯Ù‡Ø¨Ø§Ù†', 'Ù…Ø¬Ù…Ø¹ ØªØ´Ø®ÛŒØµ'],
                'patterns': [r'Ø§ØµÙ„\s*\d+', r'Ù‚Ø§Ù†ÙˆÙ†\s*Ø§Ø³Ø§Ø³ÛŒ'],
                'weight': 1.0,
                'min_confidence': 0.8
            },
            'Ù‚Ø§Ù†ÙˆÙ†_Ø¹Ø§Ø¯ÛŒ': {
                'keywords': ['Ù‚Ø§Ù†ÙˆÙ†', 'Ù…Ù‚Ø±Ø±Ø§Øª', 'Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡', 'Ù…Ø§Ø¯Ù‡', 'ØªØ¨ØµØ±Ù‡', 'ÙØµÙ„', 'Ù…ØµÙˆØ¨Ù‡'],
                'patterns': [r'Ù…Ø§Ø¯Ù‡\s*\d+', r'ØªØ¨ØµØ±Ù‡\s*\d*', r'Ù‚Ø§Ù†ÙˆÙ†\s+[Ø¢-ÛŒ\s]{5,}'],
                'weight': 0.95,
                'min_confidence': 0.7
            },
            'Ø¯Ø§Ø¯Ù†Ø§Ù…Ù‡': {
                'keywords': ['Ø¯Ø§Ø¯Ù†Ø§Ù…Ù‡', 'Ø±Ø§ÛŒ', 'Ø­Ú©Ù…', 'Ù‚Ø±Ø§Ø±', 'Ø¯Ø§Ø¯Ú¯Ø§Ù‡', 'Ù‚Ø§Ø¶ÛŒ', 'Ù…Ø­Ú©Ù…Ù‡'],
                'patterns': [r'Ø¯Ø§Ø¯Ù†Ø§Ù…Ù‡\s*Ø´Ù…Ø§Ø±Ù‡', r'Ø±Ø§ÛŒ\s*Ø´Ù…Ø§Ø±Ù‡', r'Ø­Ú©Ù…\s*Ø¨Ù‡'],
                'weight': 0.90,
                'min_confidence': 0.7
            },
            'Ø±ÙˆÛŒÙ‡_Ù‚Ø¶Ø§ÛŒÛŒ': {
                'keywords': ['Ø¢Ø±Ø§ÛŒ ÙˆØ­Ø¯Øª Ø±ÙˆÛŒÙ‡', 'Ø¯ÛŒÙˆØ§Ù† Ø¹Ø§Ù„ÛŒ', 'ØªÙØ³ÛŒØ±', 'Ø±ÙˆÛŒÙ‡', 'Ù†Ø¸Ø±ÛŒÙ‡ Ù…Ø´ÙˆØ±ØªÛŒ'],
                'patterns': [r'Ù†Ø¸Ø±ÛŒÙ‡\s*Ø´Ù…Ø§Ø±Ù‡', r'Ø±Ø§ÛŒ\s*ÙˆØ­Ø¯Øª\s*Ø±ÙˆÛŒÙ‡'],
                'weight': 0.85,
                'min_confidence': 0.6
            },
            'Ø¢Ú¯Ù‡ÛŒ_Ù‚Ø§Ù†ÙˆÙ†ÛŒ': {
                'keywords': ['Ø¢Ú¯Ù‡ÛŒ', 'Ø§Ø¹Ù„Ø§Ù†', 'ÙØ±Ø§Ø®ÙˆØ§Ù†', 'Ù…Ù†Ø§Ù‚ØµÙ‡', 'Ù…Ø²Ø§ÛŒØ¯Ù‡', 'Ø±ÙˆØ²Ù†Ø§Ù…Ù‡ Ø±Ø³Ù…ÛŒ'],
                'patterns': [r'Ø¢Ú¯Ù‡ÛŒ\s*Ø´Ù…Ø§Ø±Ù‡', r'Ø§Ø¹Ù„Ø§Ù†\s*Ù…Ù†Ø§Ù‚ØµÙ‡'],
                'weight': 0.80,
                'min_confidence': 0.6
            }
        }
        
        if TRANSFORMERS_AVAILABLE:
            self._load_models_optimized()
        else:
            logger.warning("Using rule-based classification only")

    def _load_models_optimized(self):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ HF"""
        self.load_attempts += 1
        logger.info(f"Loading optimized models (attempt {self.load_attempts})...")
        
        try:
            # Disable SSL temporarily for model downloads
            original_ssl_context = ssl._create_default_https_context
            ssl._create_default_https_context = ssl._create_unverified_context
            
            try:
                # Load embedding model (lightweight)
                self.models['embedder'] = SentenceTransformer(
                    OPTIMIZED_MODELS['embedding']['primary'],
                    device='cpu',
                    cache_folder=str(MODELS_CACHE_PATH),
                    trust_remote_code=True
                )
                logger.info("âœ… Embedding model loaded")
                
                # Load classification model with fallbacks
                for model_type in ['primary', 'fallback', 'lightweight']:
                    try:
                        model_name = OPTIMIZED_MODELS['classification'][model_type]
                        self.models['classifier'] = pipeline(
                            "text-classification",
                            model=model_name,
                            device=-1,  # CPU only
                            max_length=512,
                            truncation=True,
                            trust_remote_code=True,
                            model_kwargs={"torch_dtype": torch.float32}
                        )
                        
                        # Load tokenizer separately for better control
                        self.tokenizers['classifier'] = AutoTokenizer.from_pretrained(
                            model_name,
                            trust_remote_code=True,
                            cache_dir=str(MODELS_CACHE_PATH)
                        )
                        
                        logger.info(f"âœ… Classification model loaded: {model_type}")
                        break
                        
                    except Exception as e:
                        logger.warning(f"Failed to load {model_type} classifier: {e}")
                        if model_type == 'lightweight':  # Last attempt
                            raise e
                
                self.is_ready = True
                logger.info("ğŸ¯ HF-optimized classification system ready")
                
            finally:
                ssl._create_default_https_context = original_ssl_context
                
        except Exception as e:
            logger.error(f"Model loading failed (attempt {self.load_attempts}): {e}")
            if self.load_attempts < self.max_attempts:
                logger.info("Retrying model loading...")
                time.sleep(10)
                self._load_models_optimized()
            else:
                logger.error("Model loading permanently failed, using rule-based only")
                self.is_ready = False

    def classify_document_enhanced(self, content: str, source_info: Dict = None) -> Dict[str, Any]:
        """Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø³Ù†Ø¯"""
        if not content or not content.strip():
            return {'error': 'Empty content', 'classification': 'Ù†Ø§Ù…Ø´Ø®Øµ'}
        
        start_time = time.time()
        result = {'status': 'processing', 'models_used': []}
        
        try:
            # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…Ø­ØªÙˆØ§
            normalized_content = self.normalizer.normalize(content)
            
            # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ù…Ø­ØªÙˆØ§ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´
            words = normalized_content.split()
            if len(words) > 400:
                sample_content = ' '.join(words[:400])
            else:
                sample_content = normalized_content
            
            # Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ù‚ÙˆØ§Ù†ÛŒÙ† (Ù‡Ù…ÛŒØ´Ù‡ ÙØ¹Ø§Ù„)
            rule_result = self._enhanced_rule_based_classify(normalized_content, source_info)
            result['rule_based'] = rule_result
            result['models_used'].append('rule_based')
            
            # Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø§ Ù…Ø¯Ù„ transformer (Ø¯Ø± ØµÙˆØ±Øª Ø¢Ù…Ø§Ø¯Ú¯ÛŒ)
            if self.is_ready and 'classifier' in self.models:
                try:
                    transformer_result = self.models['classifier'](sample_content)
                    result['transformer'] = transformer_result[:3]  # Top 3 predictions
                    result['models_used'].append('transformer')
                except Exception as e:
                    logger.warning(f"Transformer classification failed: {e}")
                    result['transformer_error'] = str(e)
            
            # ØªØ±Ú©ÛŒØ¨ Ù†ØªØ§ÛŒØ¬ Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ†
            final_classification = self._combine_classification_results(result, source_info)
            result.update(final_classification)
            
            result['processing_time'] = time.time() - start_time
            result['status'] = 'completed'
            
            return result
            
        except Exception as e:
            logger.error(f"Classification error: {e}")
            return {
                'error': str(e),
                'classification': 'Ø®Ø·Ø§',
                'confidence': 0.0,
                'status': 'error',
                'processing_time': time.time() - start_time
            }

    def _enhanced_rule_based_classify(self, content: str, source_info: Dict = None) -> Dict[str, Any]:
        """Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ù‚ÙˆØ§Ù†ÛŒÙ†"""
        scores = {}
        
        # Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ù†Ø¨Ø¹
        if source_info:
            source_category = source_info.get('category', '')
            if source_category in self.enhanced_categories:
                scores[source_category] = 0.3  # Ø§Ù…ØªÛŒØ§Ø² Ù¾Ø§ÛŒÙ‡ Ø§Ø² Ù…Ù†Ø¨Ø¹
        
        # Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ùˆ Ø§Ù„Ú¯ÙˆÙ‡Ø§
        for category, config in self.enhanced_categories.items():
            category_score = scores.get(category, 0.0)
            
            # Ø§Ù…ØªÛŒØ§Ø² Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ
            keyword_matches = sum(1 for keyword in config['keywords'] 
                                if keyword in content)
            if keyword_matches > 0:
                category_score += (keyword_matches / len(config['keywords'])) * 0.4
            
            # Ø§Ù…ØªÛŒØ§Ø² Ø§Ù„Ú¯ÙˆÙ‡Ø§
            pattern_matches = sum(1 for pattern in config['patterns'] 
                                if re.search(pattern, content))
            if pattern_matches > 0:
                category_score += (pattern_matches / len(config['patterns'])) * 0.3
            
            # Ø§Ø¹Ù…Ø§Ù„ ÙˆØ²Ù†
            scores[category] = category_score * config['weight']
        
        # Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† Ø¯Ø³ØªÙ‡
        if scores:
            best_category = max(scores.keys(), key=lambda k: scores[k])
            best_score = scores[best_category]
            min_confidence = self.enhanced_categories[best_category]['min_confidence']
            
            if best_score >= min_confidence:
                return {
                    'classification': best_category,
                    'confidence': min(best_score, 1.0),
                    'all_scores': scores,
                    'method': 'rule_based'
                }
        
        # Ù¾ÛŒØ´â€ŒÙØ±Ø¶
        return {
            'classification': 'Ù†Ø§Ù…Ø´Ø®Øµ',
            'confidence': 0.1,
            'all_scores': scores,
            'method': 'rule_based'
        }

    def _combine_classification_results(self, results: Dict, source_info: Dict = None) -> Dict[str, Any]:
        """ØªØ±Ú©ÛŒØ¨ Ù†ØªØ§ÛŒØ¬ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø®ØªÙ„Ù"""
        rule_result = results.get('rule_based', {})
        transformer_result = results.get('transformer', [])
        
        # Ø§Ú¯Ø± ÙÙ‚Ø· rule-based Ø¯Ø§Ø±ÛŒÙ…
        if not transformer_result:
            return {
                'classification': rule_result.get('classification', 'Ù†Ø§Ù…Ø´Ø®Øµ'),
                'confidence': rule_result.get('confidence', 0.1),
                'method': 'rule_based_only'
            }
        
        # ØªØ±Ú©ÛŒØ¨ Ù†ØªØ§ÛŒØ¬
        rule_confidence = rule_result.get('confidence', 0.0)
        transformer_confidence = transformer_result[0].get('score', 0.0) if transformer_result else 0.0
        
        # Ø§Ú¯Ø± rule-based Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¨Ø§Ù„Ø§ÛŒÛŒ Ø¯Ø§Ø±Ø¯ØŒ Ø¢Ù† Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†
        if rule_confidence > 0.7:
            return {
                'classification': rule_result.get('classification'),
                'confidence': rule_confidence,
                'method': 'rule_based_high_confidence',
                'transformer_backup': transformer_result[0] if transformer_result else None
            }
        
        # Ø§Ú¯Ø± transformer Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¨Ø§Ù„Ø§ÛŒÛŒ Ø¯Ø§Ø±Ø¯
        if transformer_confidence > 0.8:
            return {
                'classification': self._map_transformer_label(transformer_result[0]['label']),
                'confidence': transformer_confidence,
                'method': 'transformer_high_confidence',
                'rule_backup': rule_result
            }
        
        # ØªØ±Ú©ÛŒØ¨ ÙˆØ²Ù†â€ŒØ¯Ø§Ø±
        combined_confidence = (rule_confidence * 0.6) + (transformer_confidence * 0.4)
        
        if rule_confidence >= transformer_confidence:
            return {
                'classification': rule_result.get('classification'),
                'confidence': combined_confidence,
                'method': 'combined_rule_preferred'
            }
        else:
            return {
                'classification': self._map_transformer_label(transformer_result[0]['label']),
                'confidence': combined_confidence,
                'method': 'combined_transformer_preferred'
            }

    def _map_transformer_label(self, label: str) -> str:
        """Ù†Ú¯Ø§Ø´Øª Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ Ù…Ø¯Ù„ transformer Ø¨Ù‡ Ø¯Ø³ØªÙ‡â€ŒÙ‡Ø§ÛŒ Ù…Ø­Ù„ÛŒ"""
        # Ù†Ú¯Ø§Ø´Øª Ø§Ø­ØªÙ…Ø§Ù„ÛŒ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ
        mapping = {
            'LEGAL': 'Ù‚Ø§Ù†ÙˆÙ†_Ø¹Ø§Ø¯ÛŒ',
            'CONSTITUTIONAL': 'Ù‚Ø§Ù†ÙˆÙ†_Ø§Ø³Ø§Ø³ÛŒ',
            'JUDGMENT': 'Ø¯Ø§Ø¯Ù†Ø§Ù…Ù‡',
            'REGULATION': 'Ù…Ù‚Ø±Ø±Ø§Øª',
            'NOTICE': 'Ø¢Ú¯Ù‡ÛŒ_Ù‚Ø§Ù†ÙˆÙ†ÛŒ',
            'PROCEDURE': 'Ø±ÙˆÛŒÙ‡_Ù‚Ø¶Ø§ÛŒÛŒ'
        }
        
        return mapping.get(label, label)

    def generate_embeddings(self, text: str) -> Optional[List[float]]:
        """ØªÙˆÙ„ÛŒØ¯ embeddings Ø¨Ø±Ø§ÛŒ Ù…ØªÙ†"""
        if not self.is_ready or 'embedder' not in self.models:
            return None
        
        try:
            # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø·ÙˆÙ„ Ù…ØªÙ†
            words = text.split()
            if len(words) > 300:
                text = ' '.join(words[:300])
            
            embeddings = self.models['embedder'].encode(text, convert_to_numpy=True)
            return embeddings.tolist()
            
        except Exception as e:
            logger.error(f"Embedding generation failed: {e}")
            return None

    def calculate_similarity(self, text1: str, text2: str) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ø¨ÛŒÙ† Ø¯Ùˆ Ù…ØªÙ†"""
        try:
            if not self.is_ready or 'embedder' not in self.models:
                # Fallback to simple word overlap
                return self._simple_similarity(text1, text2)
            
            embeddings = self.models['embedder'].encode([text1, text2])
            similarity = self.models['embedder'].similarity(embeddings[0:1], embeddings[1:2])
            return float(similarity[0][0])
            
        except Exception as e:
            logger.error(f"Similarity calculation failed: {e}")
            return self._simple_similarity(text1, text2)

    def _simple_similarity(self, text1: str, text2: str) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ø³Ø§Ø¯Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ù„Ù…Ø§Øª Ù…Ø´ØªØ±Ú©"""
        try:
            words1 = set(text1.split())
            words2 = set(text2.split())
            
            if not words1 or not words2:
                return 0.0
            
            intersection = words1.intersection(words2)
            union = words1.union(words2)
            
            return len(intersection) / len(union) if union else 0.0
            
        except Exception as e:
            logger.error(f"Simple similarity calculation failed: {e}")
            return 0.0

    def extract_legal_entities(self, content: str) -> Dict[str, List[str]]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø­Ù‚ÙˆÙ‚ÛŒ Ø§Ø² Ù…ØªÙ†"""
        entities = {
            'laws': [],
            'articles': [],
            'courts': [],
            'judges': [],
            'cases': [],
            'dates': [],
            'amounts': []
        }
        
        try:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ù…Ø§Ø±Ù‡ Ù…ÙˆØ§Ø¯
            articles = re.findall(r'Ù…Ø§Ø¯Ù‡\s*(\d+)', content)
            entities['articles'] = list(set(articles))
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‚ÙˆØ§Ù†ÛŒÙ†
            laws = re.findall(r'Ù‚Ø§Ù†ÙˆÙ†\s+([Ø¢-ÛŒ\s]{5,30})', content)
            entities['laws'] = list(set(laws))
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ø¯Ú¯Ø§Ù‡â€ŒÙ‡Ø§
            courts = re.findall(r'Ø¯Ø§Ø¯Ú¯Ø§Ù‡\s+([Ø¢-ÛŒ\s]{5,30})', content)
            entities['courts'] = list(set(courts))
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ§Ø±ÛŒØ®â€ŒÙ‡Ø§
            dates = re.findall(r'\d{4}/\d{1,2}/\d{1,2}|\d{1,2}/\d{1,2}/\d{4}', content)
            entities['dates'] = list(set(dates))
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¨Ø§Ù„Øº
            amounts = re.findall(r'\d+(?:ØŒ\d{3})*\s*(?:Ø±ÛŒØ§Ù„|ØªÙˆÙ…Ø§Ù†|Ø¯Ø±Ù‡Ù…)', content)
            entities['amounts'] = list(set(amounts))
            
            return entities
            
        except Exception as e:
            logger.error(f"Entity extraction failed: {e}")
            return entities

    def get_model_status(self) -> Dict[str, Any]:
        """Ø¯Ø±ÛŒØ§ÙØª ÙˆØ¶Ø¹ÛŒØª Ù…Ø¯Ù„â€ŒÙ‡Ø§"""
        return {
            'is_ready': self.is_ready,
            'transformers_available': TRANSFORMERS_AVAILABLE,
            'load_attempts': self.load_attempts,
            'models_loaded': list(self.models.keys()),
            'cache_path': str(MODELS_CACHE_PATH),
            'categories_count': len(self.enhanced_categories)
        }
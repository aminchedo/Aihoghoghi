#!/usr/bin/env python3
"""
Real HuggingFace AI System - Working implementation without heavy dependencies
Uses HuggingFace API directly for real Persian BERT analysis
"""

import requests
import json
import time
import re
from datetime import datetime
from typing import Dict, List, Any, Optional

class HuggingFaceRealAI:
    def __init__(self, api_key: str = None):
        """Initialize real HuggingFace AI system"""
        
        # HuggingFace API configuration
        self.api_key = api_key or "hf_your_token_here"  # User should set this
        self.base_url = "https://api-inference.huggingface.co"
        
        # Real Persian models that work
        self.models = {
            'persian_bert': 'HooshvareLab/bert-fa-base-uncased',
            'persian_sentiment': 'persiannlp/persian-sentiment-analysis',
            'text_classification': 'facebook/bart-large-mnli',
            'sentence_similarity': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
        }
        
        # Real Persian legal categories
        self.legal_categories = {
            'Ù‚Ø¶Ø§ÛŒÛŒ': {
                'keywords': ['Ø¯Ø§Ø¯Ú¯Ø§Ù‡', 'Ù‚Ø§Ø¶ÛŒ', 'Ø­Ú©Ù…', 'Ø±Ø£ÛŒ', 'Ø¯Ø§Ø¯Ø±Ø³ÛŒ', 'Ù…Ø­Ø§Ú©Ù…Ù‡', 'Ø¯Ø§Ø¯Ø³ØªØ§Ù†'],
                'description': 'Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø§Ù…ÙˆØ± Ù‚Ø¶Ø§ÛŒÛŒ Ùˆ Ø¯Ø§Ø¯Ú¯Ø§Ù‡â€ŒÙ‡Ø§'
            },
            'Ø§Ø¯Ø§Ø±ÛŒ': {
                'keywords': ['ÙˆØ²Ø§Ø±Øª', 'Ø³Ø§Ø²Ù…Ø§Ù†', 'Ø§Ø¯Ø§Ø±Ù‡', 'Ù…Ø¯ÛŒØ±ÛŒØª', 'Ø®Ø¯Ù…Ø§Øª', 'Ù…Ù‚Ø±Ø±Ø§Øª'],
                'description': 'Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø§Ù…ÙˆØ± Ø§Ø¯Ø§Ø±ÛŒ Ùˆ Ø¯ÙˆÙ„ØªÛŒ'
            },
            'Ù‚Ø§Ù†ÙˆÙ†ÛŒ': {
                'keywords': ['Ù‚Ø§Ù†ÙˆÙ†', 'Ù…Ø§Ø¯Ù‡', 'Ø¨Ù†Ø¯', 'ØªØ¨ØµØ±Ù‡', 'Ù…Ø¬Ù„Ø³', 'Ø´ÙˆØ±Ø§'],
                'description': 'Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ù‚ÙˆØ§Ù†ÛŒÙ† Ùˆ Ù…Ù‚Ø±Ø±Ø§Øª'
            },
            'Ù…Ø§Ù„ÛŒ': {
                'keywords': ['Ù…Ø§Ù„ÛŒØ§Øª', 'Ø¨ÙˆØ¯Ø¬Ù‡', 'Ø¯Ø±Ø¢Ù…Ø¯', 'Ù‡Ø²ÛŒÙ†Ù‡', 'Ù¾Ø±Ø¯Ø§Ø®Øª', 'Ø­Ù‚ÙˆÙ‚'],
                'description': 'Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø§Ù…ÙˆØ± Ù…Ø§Ù„ÛŒ Ùˆ Ø§Ù‚ØªØµØ§Ø¯ÛŒ'
            }
        }
        
        # Session for API calls
        self.session = requests.Session()
        self.session.headers.update({
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json'
        })
        
        # Stats
        self.stats = {
            'api_calls': 0,
            'successful_calls': 0,
            'failed_calls': 0,
            'total_analyzed': 0
        }
    
    def classify_text_with_hf(self, text: str, max_length: int = 512) -> Dict[str, Any]:
        """Real HuggingFace text classification"""
        
        if len(text) > max_length:
            text = text[:max_length]
        
        try:
            self.stats['api_calls'] += 1
            
            # Try Persian BERT model
            url = f"{self.base_url}/models/{self.models['persian_bert']}"
            
            payload = {
                "inputs": text,
                "options": {"wait_for_model": True}
            }
            
            response = self.session.post(url, json=payload, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                self.stats['successful_calls'] += 1
                
                return {
                    'success': True,
                    'model_used': 'persian_bert',
                    'result': result,
                    'api_response': True
                }
            else:
                # Fallback to rule-based
                return self._fallback_classification(text)
                
        except Exception as e:
            print(f"ğŸ”„ HF API failed, using fallback: {str(e)[:50]}")
            return self._fallback_classification(text)
    
    def _fallback_classification(self, text: str) -> Dict[str, Any]:
        """Fallback rule-based classification when API fails"""
        
        category_scores = {}
        
        for category, data in self.legal_categories.items():
            score = 0
            found_keywords = []
            
            for keyword in data['keywords']:
                count = text.count(keyword)
                if count > 0:
                    score += count
                    found_keywords.append({'keyword': keyword, 'count': count})
            
            if score > 0:
                category_scores[category] = {
                    'score': score,
                    'keywords': found_keywords,
                    'confidence': min(score / 10, 1.0)
                }
        
        if category_scores:
            primary_category = max(category_scores.keys(), 
                                 key=lambda k: category_scores[k]['score'])
            confidence = category_scores[primary_category]['confidence']
        else:
            primary_category = 'Ø¹Ù…ÙˆÙ…ÛŒ'
            confidence = 0.1
        
        return {
            'success': True,
            'model_used': 'rule_based_fallback',
            'primary_category': primary_category,
            'confidence': round(confidence, 2),
            'category_scores': category_scores,
            'api_response': False
        }
    
    def extract_entities_real(self, text: str) -> Dict[str, List[str]]:
        """Real entity extraction using regex patterns"""
        
        entities = {}
        
        # Real patterns for Persian legal text
        patterns = {
            'ØªØ§Ø±ÛŒØ®_Ø´Ù…Ø³ÛŒ': r'\d{4}/\d{1,2}/\d{1,2}',
            'ØªØ§Ø±ÛŒØ®_Ù‚Ù…Ø±ÛŒ': r'\d{1,2}/\d{1,2}/\d{4}',
            'Ø´Ù…Ø§Ø±Ù‡_Ù¾Ø±ÙˆÙ†Ø¯Ù‡': r'Ù¾Ø±ÙˆÙ†Ø¯Ù‡\s*Ø´Ù…Ø§Ø±Ù‡\s*[\d\-/]+|Ø´Ù…Ø§Ø±Ù‡\s*[\d\-/]+',
            'Ù…Ø¨Ù„Øº_Ø±ÛŒØ§Ù„': r'\d{1,3}(?:,\d{3})*\s*Ø±ÛŒØ§Ù„',
            'Ù…Ø¨Ù„Øº_ØªÙˆÙ…Ø§Ù†': r'\d{1,3}(?:,\d{3})*\s*ØªÙˆÙ…Ø§Ù†',
            'Ù…Ø§Ø¯Ù‡_Ù‚Ø§Ù†ÙˆÙ†': r'Ù…Ø§Ø¯Ù‡\s*\d+|Ø¨Ù†Ø¯\s*\d+',
            'Ù†Ø§Ù…_Ø¯Ø§Ø¯Ú¯Ø§Ù‡': r'Ø¯Ø§Ø¯Ú¯Ø§Ù‡\s+[\u0600-\u06FF\s]+',
            'Ù†Ø§Ù…_Ø´Ø®Øµ': r'Ø¢Ù‚Ø§ÛŒ\s+[\u0600-\u06FF]+|Ø®Ø§Ù†Ù…\s+[\u0600-\u06FF]+',
            'Ø´Ù…Ø§Ø±Ù‡_Ù…Ù„ÛŒ': r'\d{10}',
            'Ú©Ø¯_Ù¾Ø³ØªÛŒ': r'\d{10}|\d{5}-\d{5}'
        }
        
        for entity_type, pattern in patterns.items():
            matches = re.findall(pattern, text)
            if matches:
                # Remove duplicates and clean
                unique_matches = list(set([match.strip() for match in matches]))
                entities[entity_type] = unique_matches
        
        return entities
    
    def analyze_sentence_structure(self, text: str) -> Dict[str, Any]:
        """Real sentence structure analysis"""
        
        # Split into sentences
        sentences = re.split(r'[.!?ØŸ]', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        # Analyze each sentence
        sentence_analysis = []
        
        for i, sentence in enumerate(sentences):
            if len(sentence) < 10:
                continue
                
            # Simple sentence scoring
            words = sentence.split()
            persian_words = [w for w in words if re.match(r'[\u0600-\u06FF]+', w)]
            
            # Check for legal importance
            legal_score = 0
            for category_data in self.legal_categories.values():
                for keyword in category_data['keywords']:
                    if keyword in sentence:
                        legal_score += 1
            
            sentence_analysis.append({
                'sentence_number': i + 1,
                'text': sentence,
                'word_count': len(words),
                'persian_word_count': len(persian_words),
                'legal_importance_score': legal_score,
                'length_category': 'Ú©ÙˆØªØ§Ù‡' if len(words) < 5 else 'Ù…ØªÙˆØ³Ø·' if len(words) < 15 else 'Ø·ÙˆÙ„Ø§Ù†ÛŒ'
            })
        
        return {
            'total_sentences': len(sentence_analysis),
            'sentences': sentence_analysis,
            'average_sentence_length': sum(s['word_count'] for s in sentence_analysis) / len(sentence_analysis) if sentence_analysis else 0,
            'high_importance_sentences': len([s for s in sentence_analysis if s['legal_importance_score'] > 2])
        }
    
    def connect_related_sentences(self, sentences: List[Dict]) -> List[Dict]:
        """Connect related sentences based on content similarity"""
        
        connected_groups = []
        processed = set()
        
        for i, sentence in enumerate(sentences):
            if i in processed:
                continue
            
            # Find related sentences
            related = [sentence]
            sentence_words = set(sentence['text'].split())
            
            for j, other_sentence in enumerate(sentences[i+1:], i+1):
                if j in processed:
                    continue
                
                other_words = set(other_sentence['text'].split())
                
                # Calculate word overlap
                overlap = len(sentence_words & other_words)
                overlap_ratio = overlap / min(len(sentence_words), len(other_words))
                
                if overlap_ratio > 0.3:  # 30% word overlap
                    related.append(other_sentence)
                    processed.add(j)
            
            if len(related) > 1:
                connected_groups.append({
                    'group_id': len(connected_groups) + 1,
                    'sentences': related,
                    'connection_strength': 'Ù‚ÙˆÛŒ' if len(related) > 2 else 'Ù…ØªÙˆØ³Ø·',
                    'topic_keywords': self._extract_common_keywords([s['text'] for s in related])
                })
            
            processed.add(i)
        
        return connected_groups
    
    def _extract_common_keywords(self, texts: List[str]) -> List[str]:
        """Extract common keywords from related texts"""
        
        # Combine all texts
        combined = ' '.join(texts)
        
        # Extract Persian words
        words = re.findall(r'[\u0600-\u06FF]+', combined)
        
        # Count frequency
        word_freq = {}
        for word in words:
            if len(word) > 2:  # Skip very short words
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # Return most frequent words
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        return [word for word, freq in sorted_words[:5] if freq > 1]
    
    def comprehensive_analysis(self, content: str, url: str = None) -> Dict[str, Any]:
        """Comprehensive real analysis combining all techniques"""
        
        print(f"ğŸ§  Comprehensive AI analysis starting...")
        start_time = time.time()
        
        # Step 1: HuggingFace classification
        classification = self.classify_text_with_hf(content)
        
        # Step 2: Real entity extraction
        entities = self.extract_entities_real(content)
        
        # Step 3: Sentence structure analysis
        sentence_analysis = self.analyze_sentence_structure(content)
        
        # Step 4: Connect related sentences
        connected_sentences = self.connect_related_sentences(sentence_analysis['sentences'])
        
        # Step 5: Calculate overall scores
        relevance_score = self._calculate_comprehensive_relevance(
            content, classification, entities, sentence_analysis
        )
        
        processing_time = time.time() - start_time
        
        # Compile comprehensive result
        result = {
            'success': True,
            'url': url,
            'content_length': len(content),
            'processing_time': round(processing_time, 2),
            'classification': classification,
            'entities': entities,
            'sentence_analysis': sentence_analysis,
            'connected_sentences': connected_sentences,
            'relevance_score': relevance_score,
            'analysis_timestamp': datetime.now().isoformat(),
            'model_info': {
                'huggingface_used': classification.get('api_response', False),
                'fallback_used': not classification.get('api_response', False),
                'total_api_calls': self.stats['api_calls']
            }
        }
        
        self.stats['total_analyzed'] += 1
        
        print(f"âœ… Analysis complete in {processing_time:.2f}s")
        print(f"ğŸ·ï¸ Category: {classification.get('primary_category', 'N/A')}")
        print(f"ğŸ” Entities: {len(entities)} types")
        print(f"ğŸ“ Sentences: {sentence_analysis['total_sentences']}")
        print(f"ğŸ”— Connected groups: {len(connected_sentences)}")
        
        return result
    
    def _calculate_comprehensive_relevance(self, content: str, classification: Dict, 
                                         entities: Dict, sentence_analysis: Dict) -> int:
        """Calculate comprehensive relevance score"""
        
        score = 0
        
        # Content length score
        if len(content) > 1000: score += 15
        if len(content) > 5000: score += 15
        if len(content) > 10000: score += 10
        
        # Classification confidence score
        confidence = classification.get('confidence', 0)
        score += confidence * 20
        
        # Entity richness score
        entity_count = sum(len(entities_list) for entities_list in entities.values())
        score += min(entity_count * 2, 20)
        
        # Sentence structure score
        high_importance = sentence_analysis.get('high_importance_sentences', 0)
        score += min(high_importance * 3, 15)
        
        # Persian content score
        persian_chars = sum(1 for char in content if '\u0600' <= char <= '\u06FF')
        if persian_chars > 100:
            score += 10
        
        return min(int(score), 100)
    
    def batch_analyze(self, contents: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        """Batch analysis of multiple contents"""
        
        print(f"ğŸ“Š Starting batch analysis of {len(contents)} items...")
        
        results = []
        
        for i, item in enumerate(contents):
            print(f"ğŸ”„ Processing {i+1}/{len(contents)}: {item.get('name', 'Unknown')}")
            
            try:
                analysis = self.comprehensive_analysis(
                    item['content'], 
                    item.get('url')
                )
                
                analysis['item_name'] = item.get('name')
                results.append(analysis)
                
                # Small delay to avoid API rate limits
                time.sleep(0.5)
                
            except Exception as e:
                print(f"âŒ Failed to analyze item {i+1}: {str(e)}")
                results.append({
                    'success': False,
                    'item_name': item.get('name'),
                    'error': str(e)
                })
        
        # Generate batch summary
        successful = sum(1 for r in results if r.get('success'))
        
        batch_summary = {
            'batch_analysis': True,
            'total_items': len(contents),
            'successful_analyses': successful,
            'success_rate': (successful / len(contents)) * 100,
            'average_relevance': sum(r.get('relevance_score', 0) for r in results if r.get('success')) / successful if successful > 0 else 0,
            'processing_time': sum(r.get('processing_time', 0) for r in results if r.get('success')),
            'results': results,
            'api_stats': self.stats
        }
        
        print(f"ğŸ“Š Batch analysis complete: {successful}/{len(contents)} successful")
        
        return batch_summary

def test_real_huggingface():
    """Test real HuggingFace system with actual content"""
    
    print("ğŸ§  TESTING REAL HUGGINGFACE AI SYSTEM")
    print("=" * 45)
    
    # Initialize AI system
    ai = HuggingFaceRealAI()
    
    # Real test content (Persian legal texts)
    test_contents = [
        {
            'name': 'Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ú¯Ø§Ù‡',
            'content': '''
            Ø¯Ø§Ø¯Ú¯Ø§Ù‡ Ø¹Ù…ÙˆÙ…ÛŒ ØªÙ‡Ø±Ø§Ù† - Ø´Ø¹Ø¨Ù‡ Û±Û°
            Ù¾Ø±ÙˆÙ†Ø¯Ù‡ Ø´Ù…Ø§Ø±Ù‡ Û±Û´Û°Û³-ÛµÛ¶Û·Û¸
            
            Ø¯Ø± Ø®ØµÙˆØµ Ø¯Ø§Ø¯Ø®ÙˆØ§Ø³Øª Ø¢Ù‚Ø§ÛŒ Ø¹Ù„ÛŒ Ø§Ø­Ù…Ø¯ÛŒ Ø¹Ù„ÛŒÙ‡ Ø®Ø§Ù†Ù… Ù…Ø±ÛŒÙ… Ø±Ø¶Ø§ÛŒÛŒ
            Ø¯Ø± Ø®ØµÙˆØµ Ù…Ø·Ø§Ù„Ø¨Ù‡ Ù…Ø¨Ù„Øº Û²Û°Û°ØŒÛ°Û°Û°ØŒÛ°Û°Û° Ø±ÛŒØ§Ù„ Ø¨Ø§Ø¨Øª Ø®Ø³Ø§Ø±Øª ÙˆØ§Ø±Ø¯Ù‡
            
            Ø¨Ø§ Ø¹Ù†Ø§ÛŒØª Ø¨Ù‡ Ù…Ø§Ø¯Ù‡ Û³Û²Û¸ Ù‚Ø§Ù†ÙˆÙ† Ù…Ø¯Ù†ÛŒ Ùˆ Ù…ÙˆØ§Ø¯ Û± Ùˆ Û² Ù‚Ø§Ù†ÙˆÙ† Ù…Ø³Ø¦ÙˆÙ„ÛŒØª Ù…Ø¯Ù†ÛŒ
            Ø¯Ø§Ø¯Ú¯Ø§Ù‡ Ù¾Ø³ Ø§Ø² Ù…Ø·Ø§Ù„Ø¹Ù‡ Ù¾Ø±ÙˆÙ†Ø¯Ù‡ Ùˆ Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø¯Ù„Ù‡ Ùˆ Ù‚Ø±Ø§Ø¦Ù† Ù…ÙˆØ¬ÙˆØ¯
            
            Ø±Ø£ÛŒ: Ù…Ø­Ú©ÙˆÙ… Ø§Ø³Øª Ø®ÙˆØ§Ù†Ø¯Ù‡ Ø¨Ù‡ Ù¾Ø±Ø¯Ø§Ø®Øª Ù…Ø¨Ù„Øº Û±ÛµÛ°ØŒÛ°Û°Û°ØŒÛ°Û°Û° Ø±ÛŒØ§Ù„
            ØªØ§Ø±ÛŒØ® Ø±Ø§ÛŒ: Û±Û´Û°Û³/Û°Û·/Û±Ûµ
            ''',
            'url': 'https://test-court.ir/verdict/123'
        },
        {
            'name': 'Ø¨Ø®Ø´Ù†Ø§Ù…Ù‡ Ø§Ø¯Ø§Ø±ÛŒ',
            'content': '''
            ÙˆØ²Ø§Ø±Øª Ú©Ø´ÙˆØ± - Ù…Ø¹Ø§ÙˆÙ†Øª Ø³ÛŒØ§Ø³ÛŒ Ø§Ù…Ù†ÛŒØªÛŒ
            Ø´Ù…Ø§Ø±Ù‡: Û±Û´Û°Û³/Û±Û²Û³Û´
            
            Ø¨Ø®Ø´Ù†Ø§Ù…Ù‡ Ø¯Ø± Ø®ØµÙˆØµ Ø§Ø¬Ø±Ø§ÛŒ Ù…Ù‚Ø±Ø±Ø§Øª Ø¬Ø¯ÛŒØ¯ Ø«Ø¨Øª Ø§Ø­ÙˆØ§Ù„
            
            Ø¨Ù‡ Ø§Ø³ØªØ­Ø¶Ø§Ø± Ù…ÛŒâ€ŒØ±Ø³Ø§Ù†Ø¯ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…ØµÙˆØ¨Ù‡ Ø´Ù…Ø§Ø±Ù‡ Û±Û²Û³ Ù‡ÛŒØ£Øª ÙˆØ²ÛŒØ±Ø§Ù†
            Ù…Ù‚Ø±Ø±Ø§Øª Ø¬Ø¯ÛŒØ¯ Ø«Ø¨Øª Ø§Ø­ÙˆØ§Ù„ Ø§Ø² ØªØ§Ø±ÛŒØ® Û±Û´Û°Û³/Û°Û¸/Û°Û± Ø§Ù„Ø²Ø§Ù…â€ŒØ¢ÙˆØ± Ø®ÙˆØ§Ù‡Ø¯ Ø¨ÙˆØ¯
            
            Ú©Ù„ÛŒÙ‡ Ø§Ø¯Ø§Ø±Ø§Øª Ø«Ø¨Øª Ø§Ø­ÙˆØ§Ù„ Ù…ÙˆØ¸Ù Ø¨Ù‡ Ø±Ø¹Ø§ÛŒØª Ù…ÙˆØ§Ø±Ø¯ Ø°ÛŒÙ„ Ù‡Ø³ØªÙ†Ø¯:
            Û±- Ø«Ø¨Øª ØªÙ…Ø§Ù…ÛŒ Ù…Ø¯Ø§Ø±Ú© Ø¯Ø± Ø³Ø§Ù…Ø§Ù†Ù‡ Ø¬Ø¯ÛŒØ¯
            Û²- ØµØ¯ÙˆØ± Ú©Ø§Ø±Øªâ€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¬Ø¯ÛŒØ¯
            Û³- Ø§Ø±Ø§Ø¦Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ù…Ø§Ù‡Ø§Ù†Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯
            ''',
            'url': 'https://test-ministry.ir/circular/456'
        }
    ]
    
    # Run comprehensive analysis
    batch_result = ai.batch_analyze(test_contents)
    
    # Print real results
    print(f"\nğŸ“Š REAL HUGGINGFACE TEST RESULTS:")
    print(f"âœ… Success Rate: {batch_result['success_rate']:.1f}%")
    print(f"ğŸ§  Successful Analyses: {batch_result['successful_analyses']}/{batch_result['total_items']}")
    print(f"ğŸ“ˆ Average Relevance: {batch_result['average_relevance']:.1f}")
    print(f"â±ï¸ Total Processing: {batch_result['processing_time']:.2f}s")
    print(f"ğŸŒ API Calls: {batch_result['api_stats']['api_calls']}")
    print(f"âœ… Successful API Calls: {batch_result['api_stats']['successful_calls']}")
    
    # Show detailed results
    print(f"\nğŸ” DETAILED ANALYSIS RESULTS:")
    for result in batch_result['results']:
        if result.get('success'):
            print(f"\nğŸ“„ {result.get('item_name')}:")
            print(f"   ğŸ·ï¸ Category: {result['classification'].get('primary_category')}")
            print(f"   ğŸ¯ Confidence: {result['classification'].get('confidence')}")
            print(f"   ğŸ“ˆ Relevance: {result.get('relevance_score')}")
            print(f"   ğŸ” Entities: {len(result.get('entities', {}))}")
            print(f"   ğŸ“ Sentences: {result['sentence_analysis']['total_sentences']}")
            print(f"   ğŸ”— Connected: {len(result.get('connected_sentences', []))}")
            print(f"   ğŸ¤– Model: {result['classification'].get('model_used')}")
    
    # Save real report
    report_file = f"real_huggingface_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(batch_result, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“„ Real report saved: {report_file}")
    
    return batch_result

if __name__ == "__main__":
    test_real_huggingface()